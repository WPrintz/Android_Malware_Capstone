{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data filtering, aggregation, and machine learning classification\n",
    "\n",
    "This notebook reads in the clues from moriarty and creates two dictionaries of event time slices : positive = malicious, negative = not-malicious.  \n",
    "\n",
    "Note: Assumes the T4_df dataframe has been already ready into memory.\n",
    "\n",
    "Once the dictionary of event time slices is created, the features of interest from T4 are fed into the filter and aggregation process to create the single table for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Moriarty Clues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userid: string (nullable = true)\n",
      " |-- uuid: double (nullable = true)\n",
      " |-- details: string (nullable = true)\n",
      " |-- action: string (nullable = true)\n",
      " |-- actiontype: integer (nullable = true)\n",
      " |-- sessiontype: integer (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- sessionid: long (nullable = true)\n",
      " |-- behavior: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "Moriarty Probe: Sampled when a Clue is recorded by the Moriarty malicious agent. See further documentation here.\n",
    " userid [string]: the user ID to whom this sample belongs to.\n",
    " uuid [int]: Unix timestamp in milliseconds of when this event occurred.\n",
    " version [string]: The version of Moriarty this clue belongs to.\n",
    " action [string]: The general action being performed by the Moriarty agent.\n",
    " actionType [{benign, malicious}]: The intent of the action being performed.\n",
    " details [string]: Details on the action performed. This field may contain additional information. The format of\n",
    "this field is:\n",
    "<details> (<data field>,...);<data value>;...\n",
    "e.g.: “Successful send to server(duration [msec],size [bytes]);440;20731”\n",
    " sessionID [int]: The ID for the on going session to which this clue belongs.\n",
    " sessionType [{benign, malicious}]: The intent of the ongoing session.\n",
    " behavior [string]: Sometimes sessions may overlap (e.g. benign game playing and a spyware service). This field\n",
    "help segregate overlapping sessions and identify their intents.\n",
    "'''\n",
    "\n",
    "\n",
    "ben_mal_map = {'benign':0, 'malicious':1}\n",
    "mapping_expr = create_map([lit(x) for x in chain(*ben_mal_map.items())])\n",
    "\n",
    "moriarty_df = sc.textFile(s3dir+'/moriartyprobe/000000_0')\\\n",
    "    .map(lambda line: line.split(\"\\t\"))\\\n",
    "    .map(lambda \\\n",
    "    (userid, uuid, details, action, actiontype, sessiontype, version, sessionid, behavior):\\\n",
    "    (userid, null_to_float(uuid)/1000,\\\n",
    "    details, action, actiontype, sessiontype, version, null_to_int(sessionid), behavior))\\\n",
    "    .toDF(['userid', 'uuid', 'details', 'action', 'actiontype', 'sessiontype', 'version', 'sessionid', 'behavior'])\\\n",
    "    .withColumn(\"actiontype\", mapping_expr.getItem(col(\"actiontype\")))\\\n",
    "    .withColumn(\"sessiontype\", mapping_expr.getItem(col(\"sessiontype\")))\\\n",
    "    .cache()\n",
    "#     (userid, datetime.datetime.fromtimestamp(null_to_float(uuid)/1000),\\\n",
    "# broadcast_df.show(1) \n",
    "print moriarty_df.printSchema()\n",
    "# moriarty_df.take(1)\n",
    "\n",
    "# temp = moriarty_df.collect()\n",
    "# len(temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Positive Class Moriarty list for conversion to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of table:  9265\n",
      "+----------+---------+----------------+----------------+------------------+\n",
      "|    userid|sessionid|           begin|             end|              diff|\n",
      "+----------+---------+----------------+----------------+------------------+\n",
      "|b63c849327|       15| 1.47151920576E9|1.471519362183E9|156.42300009727478|\n",
      "|5c1f751a99|       32|1.472359634893E9|1.472359765556E9|130.66300010681152|\n",
      "|d8d16595c6|       60|1.471847683126E9|1.471848730948E9|1047.8220000267029|\n",
      "|bca57e81a1|      131|1.472027159344E9|1.472027464325E9|304.98099994659424|\n",
      "|5b76bedcac|      180|1.471772917527E9|1.471773081442E9|163.91499996185303|\n",
      "|b63c849327|      210|1.471809056166E9|1.471809195411E9|139.24500012397766|\n",
      "|5b76bedcac|      236|1.471814936261E9|1.471815240726E9| 304.4650001525879|\n",
      "|5b76bedcac|      300|1.471861058891E9|1.471861231844E9|172.95300006866455|\n",
      "|bca57e81a1|      302|1.473048883316E9| 1.47304908467E9|201.35400009155273|\n",
      "|5b76bedcac|      329|1.471878527695E9|1.471878737009E9| 209.3140001296997|\n",
      "|2cd992fd5e|      332| 1.47179092489E9|1.471791181796E9| 256.9059998989105|\n",
      "|2cd992fd5e|      356|1.471805378131E9|1.471805711024E9|332.89299988746643|\n",
      "|b63c849327|      409| 1.47199213505E9|1.471992268962E9| 133.9119999408722|\n",
      "|5b76bedcac|      490|1.472022225614E9|1.472022412674E9|187.05999994277954|\n",
      "|831b42eaa1|      511|1.473684075328E9|1.473684181368E9|106.03999996185303|\n",
      "|b63c849327|      634|1.472378296225E9|1.472378427351E9|131.12600016593933|\n",
      "|2cd992fd5e|      676|1.472005329316E9|1.472005581014E9|251.69799995422363|\n",
      "|2cd992fd5e|      768|1.472067560249E9|1.472067830373E9|270.12399983406067|\n",
      "|5b76bedcac|      769|1.472179800079E9|1.472180006216E9|206.13700008392334|\n",
      "|5b76bedcac|      789|1.472191860288E9|1.472192114449E9|254.16099977493286|\n",
      "|b63c849327|      945|1.472739971349E9|1.472740138109E9|166.75999999046326|\n",
      "|b63c849327|      991|1.472767653358E9|1.472767869085E9|215.72699999809265|\n",
      "|5b76bedcac|     1062|1.472356672835E9|1.472356835743E9| 162.9079999923706|\n",
      "|b63c849327|     1073|1.472997614468E9|1.472997745115E9| 130.6470000743866|\n",
      "|b63c849327|     1087|1.473006050425E9|1.473006223009E9|172.58400011062622|\n",
      "|b63c849327|     1136|1.473071970119E9|1.473072102985E9| 132.8659999370575|\n",
      "|5b76bedcac|     1148|1.472401061299E9|1.472401282929E9|221.62999987602234|\n",
      "|5b76bedcac|     1295|1.472489516261E9|1.472489727722E9|211.46099996566772|\n",
      "|b63c849327|     1314|1.473325784635E9|1.473325938437E9|153.80200004577637|\n",
      "|b63c849327|     1384|1.473367942143E9| 1.47336810264E9|160.49700021743774|\n",
      "+----------+---------+----------------+----------------+------------------+\n",
      "only showing top 30 rows\n",
      "\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "# Look for Begin Encrypt Download string\n",
    "temp_table1 = moriarty_df\\\n",
    "    .filter(moriarty_df.version == '8.0')\\\n",
    "    .filter(moriarty_df.actiontype == 1)\\\n",
    "    .where(\\\n",
    "           funcs.col(\"details\")\\\n",
    "           .rlike(\"{}.*{}\".format(\"Begin\",\"Download\"))\\\n",
    "           )\\\n",
    "    .withColumnRenamed('uuid', 'begin')\\\n",
    "    .alias('temp_table1')\n",
    "\n",
    "# Look for End Encrypt Download string\n",
    "temp_table2 = moriarty_df\\\n",
    "    .filter(moriarty_df.version == '8.0')\\\n",
    "    .filter(moriarty_df.actiontype == 1)\\\n",
    "    .where(\\\n",
    "           funcs.col(\"details\")\\\n",
    "           .rlike(\"{}.*{}\".format(\"End\",\"Download\"))\\\n",
    "           )\\\n",
    "    .withColumnRenamed('uuid', 'end')\\\n",
    "    .alias('temp_table2')\n",
    "  \n",
    "# Join tables \n",
    "cond = [temp_table1.userid == temp_table2.userid, temp_table1.sessionid == temp_table2.sessionid]\n",
    "temp_table3 = temp_table1.join(temp_table2, cond, how='inner')\\\n",
    "    .select(['temp_table1.userid', 'temp_table1.sessionid', 'temp_table1.begin','temp_table2.end'])\\\n",
    "    .where((temp_table2.end-temp_table1.begin) >= 100)\\\n",
    "    .where((temp_table2.end-temp_table1.begin) <= 1200)\\\n",
    "    .cache()\n",
    "\n",
    "# add begin to end time interval column\n",
    "temp_table3 = temp_table3\\\n",
    "    .withColumn('diff', temp_table3.end-temp_table3.begin)\\\n",
    "\n",
    "# Drop any duplicates (due to recording errors)\n",
    "temp_table4 = temp_table3\\\n",
    "    .orderBy('end')\\\n",
    "    .dropDuplicates(['sessionid', 'begin']).cache()\n",
    "print \"Length of table: \", temp_table4.count()\n",
    "temp_table4.show(30)\n",
    "\n",
    "# dump to list for conversion to dictionary\n",
    "moriarty_temp  = temp_table4.collect()   \n",
    "\n",
    "\n",
    "'''\n",
    "Convert moriarty_temp into a dictionary\n",
    "\n",
    "Format : \n",
    "{userid : [(sessionid, begin, end, diff), (sessionid, begin, end, diff), ...]}\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "moriarty_pos_dict = defaultdict(list)\n",
    "for userid, sess, begin, end, diff in moriarty_temp:\n",
    "    moriarty_pos_dict[userid].append((sess, begin, end, diff))\n",
    "    \n",
    "\n",
    "print len(moriarty_pos_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create Negative class Moriarty data into dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of table:  16236\n",
      "+----------+---------+----------------+----------------+------+\n",
      "|userid    |sessionid|begin           |end             |diff  |\n",
      "+----------+---------+----------------+----------------+------+\n",
      "|1a1a12314b|1        |1.470783693459E9|1.470784893459E9|1200.0|\n",
      "|1775bd8416|1        |1.470814715232E9|1.470815915232E9|1200.0|\n",
      "|f68d404dec|1        |1.470814853381E9|1.470816053381E9|1200.0|\n",
      "|a4f29791af|1        |1.470814905391E9|1.470816105391E9|1200.0|\n",
      "|55bd790216|1        |1.470814928378E9|1.470816128378E9|1200.0|\n",
      "|8960d74df5|1        |1.470815268305E9|1.470816468305E9|1200.0|\n",
      "|55153967c4|1        |1.47081585273E9 |1.47081705273E9 |1200.0|\n",
      "|b63c849327|1        |1.470818860606E9|1.470820060606E9|1200.0|\n",
      "|1775bd8416|1        |1.470822184897E9|1.470823384897E9|1200.0|\n",
      "|a4f29791af|1        |1.47082271635E9 |1.47082391635E9 |1200.0|\n",
      "+----------+---------+----------------+----------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look for Begin Encrypt Download string\n",
    "temp_table1 = moriarty_df\\\n",
    "    .filter(moriarty_df.version == '8.0')\\\n",
    "    .filter(moriarty_df.actiontype == 1)\\\n",
    "    .where(\\\n",
    "           funcs.col(\"details\")\\\n",
    "           .rlike(\"{}.*{}\".format(\"Establishing\",\"C&C\"))\\\n",
    "           )\\\n",
    "    .withColumn('begin', moriarty_df.uuid-1210)\\\n",
    "    .withColumn('end', moriarty_df.uuid-10)\\\n",
    "    .withColumn('diff', lit(1200.0))\\\n",
    "    .select(['userid', 'sessionid', 'begin', 'end', 'diff'])\n",
    "\n",
    "temp_table1.cache()\n",
    "\n",
    "print \"Length of table: \", temp_table1.count()\n",
    "temp_table1.show(10, truncate=False)\n",
    "\n",
    "# dump to list for conversion to dictionary\n",
    "moriarty_neg_temp  = temp_table1.collect()   \n",
    "\n",
    "'''\n",
    "Convert moriarty_neg_temp into a dictionary\n",
    "\n",
    "Format : \n",
    "{userid : [(sessionid, begin, end, diff), (sessionid, begin, end, diff), ...]}\n",
    "\n",
    "'''\n",
    "\n",
    "from collections import defaultdict\n",
    "moriarty_neg_dict = defaultdict(list)\n",
    "for userid, sess, begin, end, diff in moriarty_neg_temp:\n",
    "    moriarty_neg_dict[userid].append((sess, begin, end, diff))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join all interesting features into one aggregated data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userid: string (nullable = true)\n",
      " |-- eventid: long (nullable = true)\n",
      " |-- max_external_freebytes: long (nullable = true)\n",
      " |-- min_external_freebytes: long (nullable = true)\n",
      " |-- avg_external_freebytes: double (nullable = true)\n",
      " |-- range_external_freebytes_: long (nullable = true)\n",
      " |-- label: integer (nullable = false)\n",
      "\n",
      "2 of 12 done\n",
      "3 of 12 done\n",
      "4 of 12 done\n",
      "5 of 12 done\n",
      "6 of 12 done\n",
      "7 of 12 done\n",
      "8 of 12 done\n",
      "9 of 12 done\n",
      "10 of 12 done\n",
      "11 of 12 done\n",
      "12 of 12 done\n",
      "root\n",
      " |-- max_internal_freebytes: long (nullable = true)\n",
      " |-- min_internal_freebytes: long (nullable = true)\n",
      " |-- avg_internal_freebytes: double (nullable = true)\n",
      " |-- range_internal_freebytes_: long (nullable = true)\n",
      " |-- max_processes: long (nullable = true)\n",
      " |-- min_processes: long (nullable = true)\n",
      " |-- avg_processes: double (nullable = true)\n",
      " |-- range_processes_: long (nullable = true)\n",
      " |-- max_battery_level: long (nullable = true)\n",
      " |-- min_battery_level: long (nullable = true)\n",
      " |-- avg_battery_level: double (nullable = true)\n",
      " |-- range_battery_level_: long (nullable = true)\n",
      " |-- max_ctxt: long (nullable = true)\n",
      " |-- min_ctxt: long (nullable = true)\n",
      " |-- avg_ctxt: double (nullable = true)\n",
      " |-- range_ctxt_: long (nullable = true)\n",
      " |-- max_active_file: long (nullable = true)\n",
      " |-- min_active_file: long (nullable = true)\n",
      " |-- avg_active_file: double (nullable = true)\n",
      " |-- range_active_file_: long (nullable = true)\n",
      " |-- max_buffers: long (nullable = true)\n",
      " |-- min_buffers: long (nullable = true)\n",
      " |-- avg_buffers: double (nullable = true)\n",
      " |-- range_buffers_: long (nullable = true)\n",
      " |-- max_function_call_interrupts_sum_cpu123: long (nullable = true)\n",
      " |-- min_function_call_interrupts_sum_cpu123: long (nullable = true)\n",
      " |-- avg_function_call_interrupts_sum_cpu123: double (nullable = true)\n",
      " |-- range_function_call_interrupts_sum_cpu123_: long (nullable = true)\n",
      " |-- max_function_call_interrupts_cpu0: long (nullable = true)\n",
      " |-- min_function_call_interrupts_cpu0: long (nullable = true)\n",
      " |-- avg_function_call_interrupts_cpu0: double (nullable = true)\n",
      " |-- range_function_call_interrupts_cpu0_: long (nullable = true)\n",
      " |-- max_total_cpu: double (nullable = true)\n",
      " |-- min_total_cpu: double (nullable = true)\n",
      " |-- avg_total_cpu: double (nullable = true)\n",
      " |-- range_total_cpu_: double (nullable = true)\n",
      " |-- max_vmallocused: long (nullable = true)\n",
      " |-- min_vmallocused: long (nullable = true)\n",
      " |-- avg_vmallocused: double (nullable = true)\n",
      " |-- range_vmallocused_: long (nullable = true)\n",
      " |-- max_kernelstack: long (nullable = true)\n",
      " |-- min_kernelstack: long (nullable = true)\n",
      " |-- avg_kernelstack: double (nullable = true)\n",
      " |-- range_kernelstack_: long (nullable = true)\n",
      " |-- userid: string (nullable = true)\n",
      " |-- eventid: long (nullable = true)\n",
      " |-- max_external_freebytes: long (nullable = true)\n",
      " |-- min_external_freebytes: long (nullable = true)\n",
      " |-- avg_external_freebytes: double (nullable = true)\n",
      " |-- range_external_freebytes_: long (nullable = true)\n",
      " |-- label: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop it!\n",
    "\n",
    "\n",
    "# test_col='external_freeblocks'  # Pct Difference: 0.897952518083\n",
    "# test_col='kernelstack'  #   Pct Difference: 0.62776162111\n",
    "# test_col='vmallocused'  # Pct Difference: 0.528246484141\n",
    "# test_col='total_cpu'  # Pct Difference: 0.188311277918\n",
    "# test_col='function_call_interrupts_sum_cpu123' # Pct Difference: 0.983517347497  (add to cpu0?)\n",
    "# test_col='buffers' #Pct Difference: 0.398077262378\n",
    "# test_col='active_file' #Pct Difference: 0.432442896118\n",
    "# test_col='ctxt' #Pct Difference: 0.969791577589\n",
    "# test_col='battery_level' #Pct Difference: 0.666923823479\n",
    "# test_col='external_availablebytes' # Pct Difference: 0.897952581288  <-- Same as freeblocks?\n",
    "# test_col='processes' # Pct Difference: 0.965331834073\n",
    "# test_col='internal_freebytes' # Pct Difference: 0.87557881831\n",
    "# test_col='function_call_interrupts_cpu0' #Pct Difference: 0.983012596777\n",
    "\n",
    "\n",
    "feature_cols=['external_freebytes', 'kernelstack', 'vmallocused', 'total_cpu', \\\n",
    "              'function_call_interrupts_cpu0', 'function_call_interrupts_sum_cpu123', \\\n",
    "              'buffers', 'active_file', 'ctxt', 'battery_level', \\\n",
    "              'processes', 'internal_freebytes']\n",
    "\n",
    "for i, test_col in enumerate(feature_cols):\n",
    "\n",
    "    # Define Positive class from T4 CPU data\n",
    "    #================================================================\n",
    "\n",
    "    userid_keys_pos = moriarty_pos_dict.keys()\n",
    "\n",
    "    def agg_uuid_pos(userid, uuid):\n",
    "        if (userid in userid_keys_pos):\n",
    "            for i, t in enumerate(moriarty_pos_dict[userid]):\n",
    "                if (uuid <= t[2]) & (uuid >= t[1]):\n",
    "                    return i\n",
    "        return -1\n",
    "    agg_uuid_pos_udf=udf(agg_uuid_pos, LongType())\n",
    "\n",
    "\n",
    "    def cume_time_pos(userid, eventid):\n",
    "        return moriarty_pos_dict[userid][eventid][3]\n",
    "    cume_time_pos_udf=udf(cume_time_pos, FloatType())\n",
    "\n",
    "\n",
    "    def session_lookup_pos(userid, eventid):\n",
    "        return moriarty_pos_dict[userid][eventid][0]\n",
    "    sess_pos_udf=udf(session_lookup_pos, LongType())\n",
    "\n",
    "\n",
    "    temp_table_master_pos = t4_df\\\n",
    "        .select('userid', 'uuid', test_col)\\\n",
    "        .withColumn('eventid', agg_uuid_pos_udf(col('userid'), col('uuid')))\\\n",
    "        .where(col('eventid') >= 0)\\\n",
    "    #     .cache()\n",
    "\n",
    "\n",
    "\n",
    "    # Define Negative class from T4 CPU data\n",
    "    #================================================================    \n",
    "\n",
    "    userid_keys_neg = moriarty_neg_dict.keys()\n",
    "\n",
    "    def agg_uuid_neg(userid, uuid):\n",
    "        if (userid in userid_keys_neg):\n",
    "            for i, t in enumerate(moriarty_neg_dict[userid]):\n",
    "                if (uuid <= t[2]) & (uuid >= t[1]):\n",
    "                    return i\n",
    "        return -1\n",
    "    agg_uuid_neg_udf=udf(agg_uuid_neg, LongType())\n",
    "\n",
    "\n",
    "    def cume_time_neg(userid, eventid):\n",
    "        return moriarty_neg_dict[userid][eventid][3]\n",
    "    cume_time_neg_udf=udf(cume_time_neg, FloatType())\n",
    "\n",
    "\n",
    "    def session_lookup_neg(userid, eventid):\n",
    "        return moriarty_neg_dict[userid][eventid][0]\n",
    "    sess_neg_udf=udf(session_lookup_neg, LongType())\n",
    "\n",
    "\n",
    "\n",
    "    temp_table_master_neg = t4_df\\\n",
    "        .select('userid', 'uuid', test_col)\\\n",
    "        .withColumn('eventid', agg_uuid_neg_udf(col('userid'), col('uuid')))\\\n",
    "        .where(col('eventid') >= 0)\\\n",
    "    #     .cache()    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Aggregate Positive class from T4 CPU data\n",
    "    #================================================================\n",
    "\n",
    "    temp_table_pos = temp_table_master_pos\\\n",
    "        .groupBy(col('userid'), col('eventid'))\\\n",
    "        .agg(funcs.max(col(test_col)), funcs.min(col(test_col)), funcs.avg(col(test_col)))\\\n",
    "        .withColumn('range_{}'.format(test_col), col('max({})'.format(test_col))-col('min({})'.format(test_col)))\\\n",
    "        .withColumnRenamed('max({})'.format(test_col), 'max_{}'.format(test_col))\\\n",
    "        .withColumnRenamed('min({})'.format(test_col), 'min_{}'.format(test_col))\\\n",
    "        .withColumnRenamed('avg({})'.format(test_col), 'avg_{}'.format(test_col))\\\n",
    "        .withColumn('label', lit(1))\n",
    "    #     .cache()\n",
    "\n",
    "\n",
    "    # Aggregate Negative class from T4 CPU data\n",
    "    #================================================================\n",
    "\n",
    "    temp_table_neg = temp_table_master_neg\\\n",
    "        .groupBy(col('userid'), col('eventid'))\\\n",
    "        .agg(funcs.max(col(test_col)), funcs.min(col(test_col)), funcs.avg(col(test_col)))\\\n",
    "        .withColumn('range_{}_'.format(test_col), col('max({})'.format(test_col))-col('min({})'.format(test_col)))\\\n",
    "        .withColumnRenamed('max({})'.format(test_col), 'max_{}'.format(test_col))\\\n",
    "        .withColumnRenamed('min({})'.format(test_col), 'min_{}'.format(test_col))\\\n",
    "        .withColumnRenamed('avg({})'.format(test_col), 'avg_{}'.format(test_col))\\\n",
    "        .withColumn('label', lit(0))\n",
    "    #     .cache()\n",
    "\n",
    "\n",
    "\n",
    "    # If first loop, copy the table and skip the rest of the join\n",
    "    #================================================================\n",
    "\n",
    "    if i == 0:\n",
    "        big_table_neg = temp_table_neg\n",
    "        big_table_pos = temp_table_pos\n",
    "\n",
    "        big_table_neg.printSchema()\n",
    "        \n",
    "        continue\n",
    "\n",
    "    # Join Positive class from T4 CPU data\n",
    "    #================================================================\n",
    "\n",
    "    temp_table_pos = temp_table_pos\\\n",
    "        .withColumnRenamed('userid', '_userid')\\\n",
    "        .withColumnRenamed('eventid', '_eventid')\\\n",
    "        .withColumnRenamed('label', '_label')\n",
    "    dropcols = ['_userid', '_eventid', '_label']\n",
    "\n",
    "    big_table_pos = temp_table_pos.join(big_table_pos, \\\n",
    "        [temp_table_pos._userid==big_table_pos.userid, temp_table_pos._eventid==big_table_pos.eventid])\\\n",
    "        .drop(*dropcols)\\\n",
    "        .cache()\n",
    "\n",
    "\n",
    "\n",
    "    # Join negative class from T4 CPU data\n",
    "    #================================================================\n",
    "\n",
    "    temp_table_neg = temp_table_neg\\\n",
    "        .withColumnRenamed('userid', '_userid')\\\n",
    "        .withColumnRenamed('eventid', '_eventid')\\\n",
    "        .withColumnRenamed('label', '_label')\n",
    "    dropcols = ['_userid', '_eventid', '_label']\n",
    "\n",
    "    big_table_neg = temp_table_neg.join(big_table_neg, \\\n",
    "        [temp_table_neg._userid==big_table_neg.userid, temp_table_neg._eventid==big_table_neg.eventid])\\\n",
    "        .drop(*dropcols)\\\n",
    "        .cache()\n",
    "\n",
    "    print \"{} of {} done\".format(i+1, len(feature_cols))\n",
    "    \n",
    "big_table_neg.printSchema()\n",
    "all_data = big_table_neg.union(big_table_pos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Import MLlib models and train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['max_internal_freebytes', 'min_internal_freebytes', 'avg_internal_freebytes', 'range_internal_freebytes_', 'avg_processes', 'range_processes_', 'max_battery_level', 'min_battery_level', 'avg_battery_level', 'range_battery_level_', 'avg_ctxt', 'range_ctxt_', 'max_active_file', 'min_active_file', 'avg_active_file', 'range_active_file_', 'max_buffers', 'min_buffers', 'avg_buffers', 'range_buffers_', 'avg_function_call_interrupts_sum_cpu123', 'range_function_call_interrupts_sum_cpu123_', 'avg_function_call_interrupts_cpu0', 'range_function_call_interrupts_cpu0_', 'avg_total_cpu', 'range_total_cpu_', 'avg_vmallocused', 'range_vmallocused_', 'avg_kernelstack', 'range_kernelstack_', 'avg_external_freebytes', 'range_external_freebytes_']\n"
     ]
    }
   ],
   "source": [
    "input_cols = [\\\n",
    " 'max_internal_freebytes',\n",
    " 'min_internal_freebytes',\n",
    " 'avg_internal_freebytes',\n",
    " 'range_internal_freebytes_',\n",
    "#  'max_processes',\n",
    "#  'min_processes',\n",
    " 'avg_processes',\n",
    " 'range_processes_',\n",
    " 'max_battery_level',\n",
    " 'min_battery_level',\n",
    " 'avg_battery_level',\n",
    " 'range_battery_level_',\n",
    "#  'max_ctxt',\n",
    "#  'min_ctxt',\n",
    " 'avg_ctxt',\n",
    " 'range_ctxt_',\n",
    " 'max_active_file',\n",
    " 'min_active_file',\n",
    " 'avg_active_file',\n",
    " 'range_active_file_',\n",
    " 'max_buffers',\n",
    " 'min_buffers',\n",
    " 'avg_buffers',\n",
    " 'range_buffers_',\n",
    "#  'max_function_call_interrupts_sum_cpu123',\n",
    "#  'min_function_call_interrupts_sum_cpu123',\n",
    " 'avg_function_call_interrupts_sum_cpu123',\n",
    " 'range_function_call_interrupts_sum_cpu123_',\n",
    "#  'max_function_call_interrupts_cpu0',\n",
    "#  'min_function_call_interrupts_cpu0',\n",
    " 'avg_function_call_interrupts_cpu0',\n",
    " 'range_function_call_interrupts_cpu0_',\n",
    "#  'max_total_cpu',\n",
    "#  'min_total_cpu',\n",
    " 'avg_total_cpu',\n",
    " 'range_total_cpu_',\n",
    "#  'max_vmallocused',\n",
    "#  'min_vmallocused',\n",
    " 'avg_vmallocused',\n",
    " 'range_vmallocused_',\n",
    "#  'max_kernelstack',\n",
    "#  'min_kernelstack',\n",
    " 'avg_kernelstack',\n",
    " 'range_kernelstack_',\n",
    "#  'max_external_freebytes',\n",
    "#  'min_external_freebytes',\n",
    " 'avg_external_freebytes',\n",
    " 'range_external_freebytes_']\n",
    "\n",
    "# all_data = sqlContext.read.load(s3dir+'t4_features.parquet').cache()\n",
    "\n",
    "print input_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data:\n",
      "\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 7238|\n",
      "|    0|12150|\n",
      "+-----+-----+\n",
      "\n",
      "test data:\n",
      "\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 1782|\n",
      "|    0| 3076|\n",
      "+-----+-----+\n",
      "\n",
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from  pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator as BCE\n",
    "\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "\n",
    "all_data_vectorized = vecAssembler.transform(all_data).select('label', 'features')\n",
    "\n",
    "#Split data into train/test sets\n",
    "training_data, test_data = all_data_vectorized.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "print \"training data:\\n\"\n",
    "training_data.groupBy('label').count().show()\n",
    "\n",
    "print \"test data:\\n\"\n",
    "test_data.groupBy('label').count().show()\n",
    "\n",
    "training_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create custom score report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_model(model_predictions, cols):\n",
    "    '''\n",
    "    input : \n",
    "        model_predictions [spark dataframe]\n",
    "        cols : list of column names in this format\n",
    "            [0] : actual_label\n",
    "            [1] : predicted_label\n",
    "            [2] : prediction_score in format [probability, 1-probability]\n",
    "    '''\n",
    "    \n",
    "\n",
    "    # make confusion matrix\n",
    "    tp = model_predictions.where(col(cols[0]) == 1).where(col(cols[1]) == 1).count()\n",
    "    tn = model_predictions.where(col(cols[0]) == 0).where(col(cols[1]) == 0).count()\n",
    "    fp = model_predictions.where(col(cols[0]) == 0).where(col(cols[1]) == 1).count()\n",
    "    fn = model_predictions.where(col(cols[0]) == 1).where(col(cols[1]) == 0).count()\n",
    "\n",
    "    print\n",
    "    print \"Confusion Matrix\"\n",
    "    print \"{:>25}{:>20}\".format(\"Pred. True\", \"Pred. False\")\n",
    "    print \"Act. True: {: >11}{: >20}\".format(tp, fp)\n",
    "    print \"Act. False: {: >10}{: >20}\".format(fn, tn)\n",
    "\n",
    "    print\n",
    "    print \"Accuracy : {0:.3f}\".format((tp + tn)/(tp + tn + fp + float(fn)))\n",
    "\n",
    "    print\n",
    "    \n",
    "    if len(cols) > 2:\n",
    "        # Score model AUC (ROC)\n",
    "        score_and_label = model_predictions.select([cols[0], cols[2]])\n",
    "        evaluator = BCE()\n",
    "        print \"Area under the ROC curve: {}\".format(evaluator.evaluate(score_and_label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Model\n",
      "========================================\n",
      "\n",
      "Confusion Matrix\n",
      "               Pred. True         Pred. False\n",
      "Act. True:        1282                 334\n",
      "Act. False:        500                2742\n",
      "\n",
      "Acccuracy : 0.828\n",
      "\n",
      "Area under the ROC curve: 0.877809667255\n"
     ]
    }
   ],
   "source": [
    "blor = LogisticRegression(featuresCol=\"features\", \\\n",
    "                          labelCol=\"label\", \\\n",
    "                          predictionCol=\"prediction\", \\\n",
    "                          probabilityCol=\"probability\", \\\n",
    "                          rawPredictionCol=\"rawPrediction\", \\\n",
    "                          maxIter=100, \\\n",
    "                          regParam=0.0, \\\n",
    "                          elasticNetParam=0.0, \\\n",
    "                          tol=1e-6, \\\n",
    "                          fitIntercept=True, \\\n",
    "                          threshold=0.5, \\\n",
    "                          standardization=True, \\\n",
    "                          aggregationDepth=2, \\\n",
    "                          family=\"auto\")\n",
    "\n",
    "blor_model = blor.fit(training_data)\n",
    "blor_predictions = blor_model.transform(test_data)\n",
    "\n",
    "print \"Binary Logistic Regression Model\"\n",
    "print \"=\"*40\n",
    "score_model(blor_predictions, ['label', 'prediction', 'rawPrediction'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix\n",
      "               Pred. True         Pred. False\n",
      "Act. True:        1687                  55\n",
      "Act. False:        157                3014\n",
      "\n",
      "Acccuracy : 0.957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbt = GBTClassifier(featuresCol=\"features\", \\\n",
    "                    labelCol=\"label\", \\\n",
    "                    predictionCol=\"prediction\", \\\n",
    "                    maxDepth=5, \\\n",
    "                    maxBins=32, \\\n",
    "                    minInstancesPerNode=1, \\\n",
    "                    minInfoGain=0.0, \\\n",
    "                    maxMemoryInMB=256, \\\n",
    "                    cacheNodeIds=False, \\\n",
    "                    checkpointInterval=10, \\\n",
    "                    lossType=\"logistic\", \\\n",
    "                    maxIter=20, \\\n",
    "                    stepSize=0.1, \\\n",
    "                    seed=None, \\\n",
    "                    subsamplingRate=1.0)\n",
    "\n",
    "print \"Gradient Boosted Trees Classification Model\"\n",
    "print \"=\"*40\n",
    "gbt_model = gbt.fit(training_data)\n",
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "\n",
    "score_model(gbt_predictions, ['label', 'prediction'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
