I used several scripts to help startup the Spark cluster as well as perform data analysis.

Because the Jupyter notebooks are too large to display in the web portal, I will split them up into more context-specific versions.


# Starting up EMR cluster

* __EMR_Launch_awscli.txt__ : Two examples of launching the EMR cluster from [aws-cli](https://aws.amazon.com/cli/) code.  The same thing can be done through the AWS EMR [web portal](https://us-west-2.console.aws.amazon.com/elasticmapreduce/home?region=us-west-2#) (US-West region), and in fact the web-portal version allows you to export the cluster creation as a CLI command.  Arguably, the web-portal version should be done at least once to ensure all permissions and settings are correct for that user prior to using the CLI.

* __bootstrap-emr.sh__ : CLI script that is called in the EMR bootstrap sequence, run on master and all core nodes. This version downloads Anaconda Python2.7 and sets up environment variables for Spark to use Anaconda.

* __jupyspark-emr.sh__ : CLI script that is run from the Spark master node.  It will enable the Spark Python environment to use Anaconda and Jupyter notebook, then launches a PySpark cluster job instance.<br><br>
Notes: <br>
Jupyter Notebook is launched in no-browser mode, and listens on port 8889.<br><br>
SSH tunnel should be setup for port 8889 between local machine and Spark master.



# File sample extraction

These scripts are used to extract sample data from log files and save to disk.  For example, in order to debug test parsing and dataframe creation without having to read large S3 files each time.

* __smallerizer_file__ : CLI script to shrink a large file by taking only the first n lines of the file in order. Assumes file is a local file.

* __smallerizer_firstn__ : CLI script to shrink a large file by taking only the first n lines of the file in order. Expected input is piped from stdin, ie if streaming from online source or very large file.

* __smallerizer_rand__ : CLI script to shrink a large file by keeping n lines of the file, with random selection of lines (does not keep lines in order).  Based on the reservoir sampling method of keeping equal-probability of lines when total file size is unknown or when streaming continuous events.  Expected input is piped from stdin, ie if streaming from online source or very large file.

# Reading in data from SherLock files on S3

* __Sherlock_pipeline.ipynb__ : Placeholder Jupyter notebook with data import functions.

# Performing data munging and analysis in Spark

* __For_EMR_clean.ipynb__ : Placeholder Jupyter notebook with data analysis & workflow functions.

# Cold start to analysis

Generally, this was my workflow for launching and working in EMR.

1.  Launch cluster through EMR_launch script  (will setup spark and  bootstrap-script install anaconda)

3.  Edit jupyspark-emr.sh to correct number of core/master CPUs and memory.

4.  scp jupyspark-emr.sh to cluster master

5.  Edit the spark conf file to change console logging from INFO to ERROR (/usr/lib/spark/conf/log4j.properties)

6.  Run bash script jupyspark-emr.sh to enable jupyter notebook access from local machine to Spark master.

7.  Setup ssh tunnel to Jupyter notebook  : <br>
    ssh -NfL 8889:localhost:8889 spark<br>
    Access Jupyter notebook

8.  Monitor cluster manager through : ssh -ND 8157 spark<br>
    (turn on foxy-proxy settings in Chrome to enable web access)<br>
    YARN Resource Manager	http://master-public-dns-name:8088/<br>
    Ganglia	http://master-public-dns-name/ganglia/<br>
