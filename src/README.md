I used several scripts to help startup the Spark cluster as well as perform data analysis.

Most analysis was performed in a large Jupyter notebook.  Because the full Jupyter notebook is too large to display in the web portal, I will split them up into more context-specific versions.  


# Starting up EMR cluster

* __EMR_Launch_awscli.txt__ : Two examples of launching the EMR cluster from [aws-cli](https://aws.amazon.com/cli/) code.  The same thing can be done through the AWS EMR [web portal](https://us-west-2.console.aws.amazon.com/elasticmapreduce/home?region=us-west-2#) (US-West region), and in fact the web-portal version allows you to export the cluster creation as a CLI command.  Arguably, the web-portal version should be done at least once to ensure all permissions and settings are correct for that user prior to using the CLI.

* __bootstrap-emr.sh__ : CLI script that is called in the EMR bootstrap sequence, run on master and all core nodes. This version downloads Anaconda Python2.7 and sets up environment variables for Spark to use Anaconda.

* __jupyspark-emr.sh__ : CLI script that is run from the Spark master node.  It will enable the Spark Python environment to use Anaconda and Jupyter notebook, then launches a PySpark cluster job instance.<br><br>
Notes: <br>
Jupyter Notebook is launched in no-browser mode, and listens on port 8889.<br><br>
SSH tunnel should be setup for port 8889 between local machine and Spark master.



# File sample extraction

These scripts are used to extract sample data from log files and save to disk.  For example, in order to debug test parsing and dataframe creation without having to read large S3 files each time.

* __smallerizer_file__ : CLI script to shrink a large file by taking only the first n lines of the file in order. Assumes file is a local file.

* __smallerizer_firstn__ : CLI script to shrink a large file by taking only the first n lines of the file in order. Expected input is piped from stdin, ie if streaming from online source or very large file.

* __smallerizer_rand__ : CLI script to shrink a large file by keeping n lines of the file, with random selection of lines (does not keep lines in order).  Based on the reservoir sampling method of keeping equal-probability of lines when total file size is unknown or when streaming data with an unknown end-point.  Expected input is piped from stdin, ie if streaming from online source or very large file.

# Reading in data from SherLock files on S3

* __Sherlock_pipeline.ipynb__ : Placeholder Jupyter notebook with data import functions.

# Performing data munging and analysis in Spark

* __For_EMR_clean.ipynb__ : Placeholder Jupyter notebook with data analysis & workflow functions.

# Cold start to analysis

Generally, this was my workflow for launching and working in EMR.

1.  Launch cluster through the proper __EMR_launch__ CLI command  (will setup spark and  bootstrap-script install anaconda)

2.  Edit __jupyspark-emr.sh__ to correct number of core/master CPUs and memory.

3.  scp __jupyspark-emr.sh__ to cluster master

4.  Edit the spark conf file to change console logging from INFO to ERROR (/usr/lib/spark/conf/log4j.properties)

5.  Run bash script __jupyspark-emr.sh__ to enable Jupyter notebook access from local machine to Spark master.

6.  Setup ssh tunnel to Jupyter notebook  : <br>
    ssh -NfL 8889:localhost:8889 spark-master-public-dns-name<br>

7.  Monitor cluster manager through : ssh -ND 8157 spark-master-public-dns-name
    (turn on [foxy-proxy settings](http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-connect-master-node-proxy.html) in Chrome to enable web access)<br><br>
    YARN Resource Manager :	http://spark-master-public-dns-name:8088/<br><br>
    Ganglia :	http://spark-master-public-dns-name/ganglia/<br>

8.  Connect to Spark master Jupyter notebook from local machine.  ie: http://localhost:8889
  <br><br>Launch a new Jupyter Python notebook or open an existing one to perform data analysis.
  <br><br>If everything is working properly, a spark context object should already reside in memory in the notebook.

9. When finished analysis, it is a good idea to terminate the EMR cluster to save money.  Note all data on cluster will be destroyed when cluster is terminated.
