{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Import MLlib models and train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['max_internal_freebytes', 'min_internal_freebytes', 'avg_internal_freebytes', 'range_internal_freebytes_', 'avg_processes', 'range_processes_', 'max_battery_level', 'min_battery_level', 'avg_battery_level', 'range_battery_level_', 'avg_ctxt', 'range_ctxt_', 'max_active_file', 'min_active_file', 'avg_active_file', 'range_active_file_', 'max_buffers', 'min_buffers', 'avg_buffers', 'range_buffers_', 'avg_function_call_interrupts_sum_cpu123', 'range_function_call_interrupts_sum_cpu123_', 'avg_function_call_interrupts_cpu0', 'range_function_call_interrupts_cpu0_', 'avg_total_cpu', 'range_total_cpu_', 'avg_vmallocused', 'range_vmallocused_', 'avg_kernelstack', 'range_kernelstack_', 'avg_external_freebytes', 'range_external_freebytes_']\n"
     ]
    }
   ],
   "source": [
    "input_cols = [\\\n",
    " 'max_internal_freebytes',\n",
    " 'min_internal_freebytes',\n",
    " 'avg_internal_freebytes',\n",
    " 'range_internal_freebytes_',\n",
    "#  'max_processes',\n",
    "#  'min_processes',\n",
    " 'avg_processes',\n",
    " 'range_processes_',\n",
    " 'max_battery_level',\n",
    " 'min_battery_level',\n",
    " 'avg_battery_level',\n",
    " 'range_battery_level_',\n",
    "#  'max_ctxt',\n",
    "#  'min_ctxt',\n",
    " 'avg_ctxt',\n",
    " 'range_ctxt_',\n",
    " 'max_active_file',\n",
    " 'min_active_file',\n",
    " 'avg_active_file',\n",
    " 'range_active_file_',\n",
    " 'max_buffers',\n",
    " 'min_buffers',\n",
    " 'avg_buffers',\n",
    " 'range_buffers_',\n",
    "#  'max_function_call_interrupts_sum_cpu123',\n",
    "#  'min_function_call_interrupts_sum_cpu123',\n",
    " 'avg_function_call_interrupts_sum_cpu123',\n",
    " 'range_function_call_interrupts_sum_cpu123_',\n",
    "#  'max_function_call_interrupts_cpu0',\n",
    "#  'min_function_call_interrupts_cpu0',\n",
    " 'avg_function_call_interrupts_cpu0',\n",
    " 'range_function_call_interrupts_cpu0_',\n",
    "#  'max_total_cpu',\n",
    "#  'min_total_cpu',\n",
    " 'avg_total_cpu',\n",
    " 'range_total_cpu_',\n",
    "#  'max_vmallocused',\n",
    "#  'min_vmallocused',\n",
    " 'avg_vmallocused',\n",
    " 'range_vmallocused_',\n",
    "#  'max_kernelstack',\n",
    "#  'min_kernelstack',\n",
    " 'avg_kernelstack',\n",
    " 'range_kernelstack_',\n",
    "#  'max_external_freebytes',\n",
    "#  'min_external_freebytes',\n",
    " 'avg_external_freebytes',\n",
    " 'range_external_freebytes_']\n",
    "\n",
    "# all_data = sqlContext.read.load(s3dir+'t4_features.parquet').cache()\n",
    "\n",
    "print input_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data:\n",
      "\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 7238|\n",
      "|    0|12150|\n",
      "+-----+-----+\n",
      "\n",
      "test data:\n",
      "\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 1782|\n",
      "|    0| 3076|\n",
      "+-----+-----+\n",
      "\n",
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from  pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator as BCE\n",
    "\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "\n",
    "all_data_vectorized = vecAssembler.transform(all_data).select('label', 'features')\n",
    "\n",
    "#Split data into train/test sets\n",
    "training_data, test_data = all_data_vectorized.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "print \"training data:\\n\"\n",
    "training_data.groupBy('label').count().show()\n",
    "\n",
    "print \"test data:\\n\"\n",
    "test_data.groupBy('label').count().show()\n",
    "\n",
    "training_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create custom score report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_model(model_predictions, cols):\n",
    "    '''\n",
    "    input : \n",
    "        model_predictions [spark dataframe]\n",
    "        cols : list of column names in this format\n",
    "            [0] : actual_label\n",
    "            [1] : predicted_label\n",
    "            [2] : prediction_score in format [probability, 1-probability]\n",
    "    '''\n",
    "    \n",
    "\n",
    "    # make confusion matrix\n",
    "    tp = model_predictions.where(col(cols[0]) == 1).where(col(cols[1]) == 1).count()\n",
    "    tn = model_predictions.where(col(cols[0]) == 0).where(col(cols[1]) == 0).count()\n",
    "    fp = model_predictions.where(col(cols[0]) == 0).where(col(cols[1]) == 1).count()\n",
    "    fn = model_predictions.where(col(cols[0]) == 1).where(col(cols[1]) == 0).count()\n",
    "\n",
    "    print\n",
    "    print \"Confusion Matrix\"\n",
    "    print \"{:>25}{:>20}\".format(\"Pred. True\", \"Pred. False\")\n",
    "    print \"Act. True: {: >11}{: >20}\".format(tp, fp)\n",
    "    print \"Act. False: {: >10}{: >20}\".format(fn, tn)\n",
    "\n",
    "    print\n",
    "    print \"Accuracy : {0:.3f}\".format((tp + tn)/(tp + tn + fp + float(fn)))\n",
    "\n",
    "    print\n",
    "    \n",
    "    if len(cols) > 2:\n",
    "        # Score model AUC (ROC)\n",
    "        score_and_label = model_predictions.select([cols[0], cols[2]])\n",
    "        evaluator = BCE()\n",
    "        print \"Area under the ROC curve: {}\".format(evaluator.evaluate(score_and_label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Model\n",
      "========================================\n",
      "\n",
      "Confusion Matrix\n",
      "               Pred. True         Pred. False\n",
      "Act. True:        1282                 334\n",
      "Act. False:        500                2742\n",
      "\n",
      "Acccuracy : 0.828\n",
      "\n",
      "Area under the ROC curve: 0.877809667255\n"
     ]
    }
   ],
   "source": [
    "blor = LogisticRegression(featuresCol=\"features\", \\\n",
    "                          labelCol=\"label\", \\\n",
    "                          predictionCol=\"prediction\", \\\n",
    "                          probabilityCol=\"probability\", \\\n",
    "                          rawPredictionCol=\"rawPrediction\", \\\n",
    "                          maxIter=100, \\\n",
    "                          regParam=0.0, \\\n",
    "                          elasticNetParam=0.0, \\\n",
    "                          tol=1e-6, \\\n",
    "                          fitIntercept=True, \\\n",
    "                          threshold=0.5, \\\n",
    "                          standardization=True, \\\n",
    "                          aggregationDepth=2, \\\n",
    "                          family=\"auto\")\n",
    "\n",
    "blor_model = blor.fit(training_data)\n",
    "blor_predictions = blor_model.transform(test_data)\n",
    "\n",
    "print \"Binary Logistic Regression Model\"\n",
    "print \"=\"*40\n",
    "score_model(blor_predictions, ['label', 'prediction', 'rawPrediction'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix\n",
      "               Pred. True         Pred. False\n",
      "Act. True:        1687                  55\n",
      "Act. False:        157                3014\n",
      "\n",
      "Acccuracy : 0.957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbt = GBTClassifier(featuresCol=\"features\", \\\n",
    "                    labelCol=\"label\", \\\n",
    "                    predictionCol=\"prediction\", \\\n",
    "                    maxDepth=5, \\\n",
    "                    maxBins=32, \\\n",
    "                    minInstancesPerNode=1, \\\n",
    "                    minInfoGain=0.0, \\\n",
    "                    maxMemoryInMB=256, \\\n",
    "                    cacheNodeIds=False, \\\n",
    "                    checkpointInterval=10, \\\n",
    "                    lossType=\"logistic\", \\\n",
    "                    maxIter=20, \\\n",
    "                    stepSize=0.1, \\\n",
    "                    seed=None, \\\n",
    "                    subsamplingRate=1.0)\n",
    "\n",
    "print \"Gradient Boosted Trees Classification Model\"\n",
    "print \"=\"*40\n",
    "gbt_model = gbt.fit(training_data)\n",
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "\n",
    "score_model(gbt_predictions, ['label', 'prediction'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
